epochs: 200
batch_size: 2
stage: finetune  # pretrain, finetune
notes: 
debug: false
load_pretrained: false

optim:
  lr: 1.0e-5

sch:
  #====== step mod ======
  name: linear  # constant, linear
  warmup_steps: 0
  #====== epoch mod ====== (我自己加的，結果沒比較好)
  # name: plateau           # 你自定義的 scheduler 名稱。這裡設定 'plateau' 代表使用 PyTorch 的 ReduceLROnPlateau（對應在 network.py 分支）。

  # # 指定監控指標是「越小越好」還是「越大越好」。
  # #- min：監控值下降視為改善（常見於 val_loss、val_MSE）。
  # #- max：監控值上升視為改善（常見於 val_R2、accuracy）。
  # mode: max          # AUC 越大越好
  # monitor: auc       # 可用: loss / acc / sens / spec / auc（或 val_xxx）   
  # factor: 0.2             # 當觸發時，學習率會乘上這個倍率。例：lr=1e-4 → 下降為 5e-5。PyTorch 預設0.1。
  # # patience: 3             # 幾個 epoch 沒進步才降 LR。PyTorch 預設10。

  # verbose: true           # 是否在 console 顯示學習率變化訊息（true 時會印出 “Reducing learning rate to …”）。
  # threshold: 0.0          # 認定「有改善」的最小差距。例：threshold=1e-3 時，只有當新 loss 比前次低超過 0.001 才算改善。
  # # 'abs' 絕對差值（absolute difference）：直接比較數值差距是否超過 threshold。例：abs(current - best) > threshold。
  # # 'rel' 相對差值（relative difference）：用比例比較改善幅度（以 best 為基準）。比較相對於 best 的差距是否超過 threshold。例：(current - best) / abs(best) > threshold。
  # threshold_mode: abs

  # # warmup_epochs: 0        # 前幾個 epoch 做 warmup（線性升到 base LR）用整數，當沒填 ratio 時才會用
  # warmup_epoch_ratio: 0.1   # 代表前 10% 的 epoch 做 warmup

paths:
  train_data: 'data/all_data/all_data_Micro_other/tr_all_data_Micro_other_A-D.pkl' # 'data/MPEA/tr1.pkl', 'data/ys_clean/tr1.pkl'
  val_data: 'data/all_data/all_data_Micro_other/vl_all_data_Micro_other_A-D.pkl' # 'data/MPEA/vl1.pkl', 'data/ys_clean/vl1.pkl'
  tokenizer: 'roberta-base' # 'm3rg-iitd/matscibert'
  pretrained: 

model:
  name_or_path: 'roberta-base' # 'm3rg-iitd/matscibert'
  num_labels: 3                # 例如 BCC / FCC / other → 3 類

early_stopping:
  enabled: true        # true 開、false 關
  patience: 20         # 幾個 epoch 沒進步就停止
  monitor: auc       # 或寫 val_auc 也可以
  mode: max
  min_delta: 0.0001    # 視為有進步的最小改善幅度
  warmup: 30            # 前幾個 epoch 不啟動早停
